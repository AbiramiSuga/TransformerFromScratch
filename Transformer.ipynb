{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Transformer from scratch using pyTorch"
      ],
      "metadata": {
        "id": "rqDFepLhOjsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*torch* for Core PyTorch library — tensors, models, autograd, etc.\n",
        "\n",
        "*torchvision* , *torchaudio* can be imported for audio/image processing."
      ],
      "metadata": {
        "id": "o0YG0IVfCgN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch tor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rehuPKHiPP3K",
        "outputId": "a4004cd5-9b30-4762-f9c9-47cdc100e492"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy"
      ],
      "metadata": {
        "id": "pUq7tzciPgvo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Head Attention Class\n",
        "\n",
        "*Self-attention focuses on finding the realtions among the words in the sequence. Multihead self-attention does the same but in multiple ways to get info like grammatical structure, semantic meaning, etc,.*\n",
        "\n",
        "1. word -> token -> q,k,v\n",
        "\n",
        "  A word is converted to tokens(smallest unit of text), then converted to vectors. Each vector is then is divided into query, key, value. Query is what is needed. Key is what we have. Value is the actual data.\n",
        "2. split heads -> what is a head and why head?\n",
        "\n",
        "  An head is an attention unit. Each head focuses on each aspect of the sentence. Number of heads === Number of ways different ways relationship between tokens are calculated.\n",
        "3. masking\n",
        "\n",
        "  Masking blocks attention to specific tokens (e.g. padding or future tokens) by setting their scores to -inf before softmax.\n",
        "4. softmax?\n",
        "\n",
        "  Softmax is used to convert attnetion scores from matmul to probabilities.\n",
        "5. what is linear projection here?\n",
        "\n",
        "  Linear projection nn.Linear is used to convert input vectors into desired dimension."
      ],
      "metadata": {
        "id": "_1bBVOAqPsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, dim_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.dim_model = dim_model\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.dim_head = dim_model // num_heads  # dim_model must be divisible by num_heads\n",
        "\n",
        "    self.query = nn.Linear(dim_model, dim_model)\n",
        "    self.key = nn.Linear(dim_model, dim_model)\n",
        "    self.value = nn.Linear(dim_model, dim_model)\n",
        "    self.output = nn.Linear(dim_model, dim_model)\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    batch_size, seq_length, dim_model = x.size()\n",
        "    return x.view(batch_size, seq_length, self.num_heads, self.dim_head).transpose(1, 2)\n",
        "\n",
        "  def combine_heads(self, x):\n",
        "    batch_size, _, seq_length, dim_head = x.size()\n",
        "    return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.dim_model)\n",
        "\n",
        "  def forward(self, q, k, v, mask=None):\n",
        "    query = self.split_heads(self.query(q))\n",
        "    key = self.split_heads(self.key(k))\n",
        "    value = self.split_heads(self.value(v))\n",
        "\n",
        "    # score calculation\n",
        "    attn_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.dim_head)\n",
        "\n",
        "    # apply mask for decoders/padding\n",
        "    if mask is not None:\n",
        "          attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    # softmax\n",
        "    attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "    # multiply probalilities with value vector and combine the heads\n",
        "    return self.output(self.combine_heads(torch.matmul(attn_probs, value)))"
      ],
      "metadata": {
        "id": "irqRbvfTPvig"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feed Forward Network\n",
        "\n",
        "*It is an MLP(Multi-layer perceptron) that is used to understand and refine each token's features individually.*\n",
        "1. What does the layers do actually?\n",
        "\n",
        "  Each hidden layer is used to expand the i/p token's dimension\n",
        "\n",
        "2. ReLu is originally used in Transformers - necessary? Any alternative?\n",
        "\n",
        "  ReLu is used to introduce non-linearity, so that the model can learn powerful mappings. GeLu(Gaussian Error Linear Unit) is smooth, probabilistic\tand used in BERT, GPT\n"
      ],
      "metadata": {
        "id": "pmI-O7HtcCZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):  # can you give multiple layers here? - yes but increases time and compute\n",
        "  def __init__(self, dim_model, dim_ff):\n",
        "    super(PositionWiseFeedForward, self).__init__()\n",
        "    self.fc1 = nn.Linear(dim_model, dim_ff) # dim_ff is usually higher than dim_model so that it can go deep and understand the token\n",
        "    self.fc2 = nn.Linear(dim_ff, dim_model)\n",
        "    self.gelu = nn.GELU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fc2(self.gelu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "GjNXflXCPwR5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional encoding\n",
        "\n",
        "*Positional encoding is used to add positional data (i.e what lies where in the sequence/sentence) as transformers process tokens parallely.*\n",
        "1. why sine and cosine ?\n",
        "\n",
        "  Sine is applied for even and cosine for odd positions to distinguish adjacent tokens\n",
        "2. how will this help for learning position ?\n",
        "\n",
        "  By this, each position will have a unique pattern across dimnesions. Without this \"She ate a cupcake after the dinner\" and \"After the dinner, she ate a cupcake\" will be similar to the model. Their semantic meaning is same but they are not identical.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kNJlOJjtbV6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pos_enc = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
        "        pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pos_enc', pos_enc.unsqueeze(0)) # just saved and used, not backpropagated.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pos_enc[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "rd10H-_MZCLg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder\n",
        "\n",
        "  One of the main components in Transformers where input is processed\n",
        "\n",
        "  1. Why layer normalization?\n",
        "\n",
        "    Stabalizes and speed-up learning\n",
        "\n",
        "  2. How q=k=v here?\n",
        "\n",
        "    Because every word/token in the input sentence requires everyother word to understand each other better. So, what we want, what we have and the actual data we have are all one and the same - the input.\n",
        "\n",
        "  3. What is a dropout?\n",
        "\n",
        "    Regularizes by randomly adding zeros in the input to avoid overfitting.\n",
        "\n",
        "  4. Residue? What & why ?\n",
        "\n",
        "    Adding back the original input to the output of a sublayer.\n",
        "    Used to maintain the input signal, helps in preserving the input pattern while transforming the same.\n",
        "\n",
        "  5. Why 2 norm and 2 dropout? Can't I reuse the same for ffn and multi-head attention?\n",
        "\n",
        "    Should not reuse that because each normalization layer maintains it's own scale and shift. If we reuse ffn and mulit-head attention layer will have to share these params which limits their learning. Same follows for dropout also - let masking be layer-specific for better learning\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1GKY9-GQZXqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, dim_model, num_heads, dim_ff, dropout=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.attn = MultiHeadAttention(dim_model, num_heads)\n",
        "        self.ffn = PositionWiseFeedForward(dim_model, dim_ff)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dim_model)\n",
        "        self.norm2 = nn.LayerNorm(dim_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.attn(x, x, x, mask)  # multi-head attention, q=k=v=x\n",
        "        x = x + self.dropout1(attn_output)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        ff_output = self.ffn(x) #feed-forward\n",
        "        x = x + self.dropout2(ff_output)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "VnbAmGt-XJyd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder\n",
        "\n",
        "  *They are crucial part of the transformers which generates output with the contextual data provided by the encoders and the data from previous layer.*\n",
        "\n",
        "  1. Masking in decoders - 2 types?\n",
        "\n",
        "    Masking is used to prevent attention to unnecessary parts - for eg. it's enough for the decoders to just know the output of it's previous layer.\n",
        "    target_mask -> look-ahead mask in self-attn\n",
        "    src-mask -> padding mask in cross-attention\n",
        "    \n",
        "  2. What is cross-attention?\n",
        "\n",
        "    Mulit-head attention applied with q= decoder's o/p and k,v = encoder's output is called cross-attention.\n",
        "  "
      ],
      "metadata": {
        "id": "RQRHHJgBxbyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, dim_model, num_heads, dim_ff, dropout=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.attn = MultiHeadAttention(dim_model, num_heads)\n",
        "    self.cross_attn = MultiHeadAttention(dim_model, num_heads)\n",
        "    self.ffn = PositionWiseFeedForward(dim_model, dim_ff)\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(dim_model)\n",
        "    self.norm2 = nn.LayerNorm(dim_model)\n",
        "    self.norm3 = nn.LayerNorm(dim_model)\n",
        "    self.dropout1 = nn.Dropout(dropout)\n",
        "    self.dropout2 = nn.Dropout(dropout)\n",
        "    self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, encoder_output, src_mask, target_mask):\n",
        "    attn_output = self.attn(x, x, x, target_mask)\n",
        "    x = x + self.dropout1(attn_output)\n",
        "    x = self.norm1(x)\n",
        "\n",
        "    cros_attn_output = self.cross_attn(x, encoder_output, encoder_output, src_mask) # cross-attention\n",
        "    x = x + self.dropout2(cros_attn_output)\n",
        "    x = self.norm2(x)\n",
        "\n",
        "    ff_output = self.ffn(x)\n",
        "    x = x + self.dropout3(ff_output)\n",
        "    x = self.norm3(x)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "2nkF2PGHxbfS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer\n",
        "\n",
        "1. Token → Embedding → Positional Encoding\n",
        "2. Encoder → Self-attention + FFN\n",
        "3. Decoder → Masked Self-attn → Cross-attn → FFN\n",
        "4. Output → Linear → Logits for prediction\n"
      ],
      "metadata": {
        "id": "jdmL6mqg42Wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, dim_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, dim_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, dim_model)\n",
        "        self.positional_encoding = PositionalEncoding(dim_model, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([Encoder(dim_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([Decoder(dim_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(dim_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        # Padding mask: 1 for non-padding tokens, 0 for padding tokens\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)  # (B, 1, 1, src_len)\n",
        "        tgt_padding_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)  # (B, 1, 1, tgt_len)\n",
        "\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = torch.tril(torch.ones((1, 1, seq_length, seq_length), device=tgt.device)).bool()  # (1, 1, tgt_len, tgt_len)\n",
        "\n",
        "        tgt_mask = tgt_padding_mask & nopeak_mask  # (B, 1, tgt_len, tgt_len)\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "K3Khbwcl445I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test the flow - model is not trained here\n",
        "\n",
        "The sample data provided and the testing script below is just to find bugs if any."
      ],
      "metadata": {
        "id": "e2VzNGEkppgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {\n",
        "    \"<pad>\": 0,\n",
        "    \"<sos>\": 1,\n",
        "    \"<eos>\": 2,\n",
        "    \"I\": 3,\n",
        "    \"am\": 4,\n",
        "    \"a\" : 5,\n",
        "    \"cupcake\": 6,\n",
        "    \"hello\": 7,\n",
        "    \"world\": 8\n",
        "}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "dim_model = 16\n",
        "num_heads = 2\n",
        "num_layers = 1\n",
        "dim_ff = 64\n",
        "max_seq_length = 10\n",
        "dropout = 0.1\n",
        "\n",
        "model = Transformer(\n",
        "    src_vocab_size=vocab_size,\n",
        "    tgt_vocab_size=vocab_size,\n",
        "    dim_model=dim_model,\n",
        "    num_heads=num_heads,\n",
        "    num_layers=num_layers,\n",
        "    d_ff=dim_ff,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dropout=dropout\n",
        ")\n",
        "\n",
        "src = torch.tensor([[3, 4, 5, 6, 2, 0]])  # shape: (1, 6)\n",
        "tgt = torch.tensor([[1, 7, 8, 2, 0, 0]])  # shape: (1, 6)\n",
        "\n",
        "with torch.no_grad():  # inference mode\n",
        "    output = model(src, tgt)\n",
        "\n",
        "print(\"Output logits shape:\", output.shape)\n",
        "predicted_ids = torch.argmax(output, dim=-1)\n",
        "print(\"Predicted token IDs:\\n\", predicted_ids)\n",
        "\n",
        "inv_vocab = {v: k for k, v in vocab.items()}\n",
        "decoded_output = [[inv_vocab[token.item()] for token in sent] for sent in predicted_ids]\n",
        "print(\"Predicted tokens:\\n\", decoded_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ooqu8GYHppET",
        "outputId": "d0b64b00-9133-4421-a155-a38d2ee6266f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output logits shape: torch.Size([1, 6, 9])\n",
            "Predicted token IDs:\n",
            " tensor([[6, 3, 3, 8, 8, 6]])\n",
            "Predicted tokens:\n",
            " [['cupcake', 'I', 'I', 'world', 'world', 'cupcake']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UOtGofkQDR91"
      }
    }
  ]
}